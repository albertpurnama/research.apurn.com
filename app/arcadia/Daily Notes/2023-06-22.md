---
tags: daily
mood: <empty>
---

## üß® Tasks
```tasks
not done
(scheduled before 2023-06-22) OR (due before 2023-06-22)
```

## üìà Stats 
*Things to celebrate on!*

### üèÜ Tasks completed

|Month|Yesterday|Today|
|------|----------|------|
|`$= dv.pages().file.tasks.where(t => t.completed).where(t => t.text.includes('‚úÖ ' + new Date(new Date('2023-06-22').setDate(1)).toISOString().slice(0, 7))).length`|`$= dv.pages().file.tasks.where(t => t.completed).where(t => t.text.includes('‚úÖ ' + new Date(new Date('2023-06-22') - 86400000).toISOString().slice(0, 10))).length`|`$= dv.pages().file.tasks.where(t => t.completed).where(t => t.text.includes('‚úÖ 2023-06-22')).length`|


## ü•≥ Done Today!
```tasks
done 2023-06-22
```


## ü™µ Log
*What's on my mind today?*


- [x] Try the `processTemplateBeforeUsage`  on TemplateCarousel. ‚è≥ 2023-06-22 üìÖ 2023-06-22 ‚úÖ 2023-06-22
- [x] Refactor `@/utils/formTransforms` ‚è≥ 2023-06-22 üìÖ 2023-06-22 ‚úÖ 2023-06-22
- [ ] 




Questions Productionizing
- As early-stage startups looking to implement AI in their service, how should we think about the cost of inferences? Do we think about the most efficient prompts since the beginning?
	- 
- Since existing LLMs like GPT-4 is really good for the majority of use cases. How do I think about fine-tuning? when should I fine-tune models?
	- No. You can get far just by prompt engineering.
- How should early-stage startups use LLMs to generate new data when they have no data early on?
	- Generate datasets dynamically, there's some tool there
	- Accumulate dataset
- Getting feedback on production, how?
	- Thumbs up and thumbs down
	- Indirect feedback (click on link and do not click)
- What data do we need to store in order to evaluate the performance of LLMs other than the user feedback on production? Should we think about this before productionizing or can it be incrementally added?
	- 
- Generate prompt from expected values:
	- Ask chatGPT if I want you to generate the following outputs, what should the prompt be?
	- 



Robert Nishihara
- Using open-source LLMs, how difficult is it to host the model? What does the infra look like? do we think of the Open source LLM as just another backend server?
	- Ray & Anyscale 
	- Huggingface
	- LangChain
- Is ray designed for training models? or is it designed running inferences?
- Ray Serve + vLLM
	- 3x faster than HuggingFace Text generation inference

What advice for founders?

ASk yourself:
- What value can you provide in the AI product? you can't simply call OpenAI API.
	- Is it the UX?
	- Is it the proprietary data?
- 

Lianmin Zheng
- 

Airbyte:
- Can be used for Notion. Maybe ask some questions on syncing between these 2 sources.
	- Can be used to¬†sync multiple databases
- 

Infrastructures:



Top 10:
- Form blank space.
- Publish Page kadang fail.



